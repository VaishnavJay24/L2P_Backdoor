### Starting TaskPrologue of job 924746 on tg072 at Sat 02 Nov 2024 01:31:39 PM CET
Running on cores 0-1,8-9,16-17,24-25 with governor ondemand
Sat Nov  2 13:31:39 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           On  |   00000000:18:00.0 Off |                    0 |
| N/A   37C    P0             27W /  250W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

| distributed init (rank 0): env://
Files already downloaded and verified
Files already downloaded and verified
Creating original model: vit_base_patch16_224
Creating model: vit_base_patch16_224
Namespace(aa=None, batch_size=16, batchwise_prompt=True, clip_grad=1.0, color_jitter=None, cooldown_epochs=10, data_path='./local_datasets/', dataset='Split-CIFAR100', decay_epochs=30, decay_rate=0.1, device='cuda', dist_backend='nccl', dist_url='env://', distributed=True, drop=0.0, drop_path=0.0, embedding_key='cls', epochs=5, eval=False, freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], global_pool='token', gpu=0, head_type='prompt', initializer='uniform', input_size=224, length=5, lr=0.03, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, min_lr=1e-05, model='vit_base_patch16_224', momentum=0.9, nb_classes=100, num_tasks=10, num_workers=4, opt='adam', opt_betas=(0.9, 0.999), opt_eps=1e-08, output_dir='./output', patience_epochs=10, pin_mem=True, predefined_key='', pretrained=True, print_freq=10, prompt_key=True, prompt_key_init='uniform', prompt_pool=True, pull_constraint=True, pull_constraint_coeff=0.1, rank=0, recount=1, reinit_optimizer=True, remode='pixel', reprob=0.0, sched='constant', seed=42, shared_prompt_key=False, shared_prompt_pool=False, shuffle=False, size=10, smoothing=0.1, subparser_name='cifar100_l2p', task_inc=False, top_k=5, train_interpolation='bicubic', train_mask=True, unscale_lr=True, use_prompt_mask=False, warmup_epochs=5, warmup_lr=1e-06, weight_decay=0.0, world_size=1)
number of params: 122980
Start training for 5 epochs
Train: Epoch[1/5]  [  0/313]  eta: 0:14:13  Lr: 0.001875  Loss: 2.3091  Acc@1: 25.0000 (25.0000)  Acc@5: 43.7500 (43.7500)  time: 2.7278  data: 0.4529  max mem: 2371
Train: Epoch[1/5]  [ 10/313]  eta: 0:02:06  Lr: 0.001875  Loss: 2.0321  Acc@1: 50.0000 (41.4773)  Acc@5: 75.0000 (73.2955)  time: 0.4162  data: 0.0413  max mem: 2372
Train: Epoch[1/5]  [ 20/313]  eta: 0:01:29  Lr: 0.001875  Loss: 1.8673  Acc@1: 50.0000 (52.0833)  Acc@5: 81.2500 (80.3571)  time: 0.1847  data: 0.0002  max mem: 2372
Train: Epoch[1/5]  [ 30/313]  eta: 0:01:15  Lr: 0.001875  Loss: 1.7414  Acc@1: 62.5000 (55.8468)  Acc@5: 93.7500 (84.2742)  time: 0.1843  data: 0.0002  max mem: 2372
Train: Epoch[1/5]  [ 40/313]  eta: 0:01:07  Lr: 0.001875  Loss: 1.3613  Acc@1: 68.7500 (60.3659)  Acc@5: 93.7500 (86.8902)  time: 0.1863  data: 0.0002  max mem: 2372
Train: Epoch[1/5]  [ 50/313]  eta: 0:01:01  Lr: 0.001875  Loss: 1.3160  Acc@1: 75.0000 (62.6225)  Acc@5: 93.7500 (88.4804)  time: 0.1862  data: 0.0002  max mem: 2372
Train: Epoch[1/5]  [ 60/313]  eta: 0:00:57  Lr: 0.001875  Loss: 1.3658  Acc@1: 75.0000 (64.0369)  Acc@5: 93.7500 (89.1393)  time: 0.1843  data: 0.0002  max mem: 2372
Train: Epoch[1/5]  [ 70/313]  eta: 0:00:53  Lr: 0.001875  Loss: 1.0895  Acc@1: 75.0000 (65.4930)  Acc@5: 93.7500 (89.9648)  time: 0.1844  data: 0.0002  max mem: 2372
Train: Epoch[1/5]  [ 80/313]  eta: 0:00:50  Lr: 0.001875  Loss: 0.9733  Acc@1: 75.0000 (67.5154)  Acc@5: 100.0000 (90.8951)  time: 0.1843  data: 0.0001  max mem: 2372
Train: Epoch[1/5]  [ 90/313]  eta: 0:00:47  Lr: 0.001875  Loss: 0.9166  Acc@1: 75.0000 (68.3379)  Acc@5: 100.0000 (91.3462)  time: 0.1843  data: 0.0001  max mem: 2372
Train: Epoch[1/5]  [100/313]  eta: 0:00:44  Lr: 0.001875  Loss: 0.9455  Acc@1: 81.2500 (69.8020)  Acc@5: 100.0000 (92.0173)  time: 0.1844  data: 0.0001  max mem: 2372
Train: Epoch[1/5]  [110/313]  eta: 0:00:42  Lr: 0.001875  Loss: 1.1735  Acc@1: 81.2500 (70.4392)  Acc@5: 100.0000 (92.2297)  time: 0.1843  data: 0.0001  max mem: 2372
Train: Epoch[1/5]  [120/313]  eta: 0:00:39  Lr: 0.001875  Loss: 0.9319  Acc@1: 75.0000 (70.8678)  Acc@5: 93.7500 (92.6653)  time: 0.1843  data: 0.0001  max mem: 2372
Train: Epoch[1/5]  [130/313]  eta: 0:00:37  Lr: 0.001875  Loss: 0.7046  Acc@1: 75.0000 (71.4218)  Acc@5: 100.0000 (93.1775)  time: 0.1845  data: 0.0001  max mem: 2372
Train: Epoch[1/5]  [140/313]  eta: 0:00:35  Lr: 0.001875  Loss: 0.3258  Acc@1: 81.2500 (72.2961)  Acc@5: 100.0000 (93.5284)  time: 0.1847  data: 0.0001  max mem: 2372
Train: Epoch[1/5]  [150/313]  eta: 0:00:32  Lr: 0.001875  Loss: 0.5384  Acc@1: 81.2500 (72.9305)  Acc@5: 100.0000 (93.8328)  time: 0.1848  data: 0.0001  max mem: 2372
Train: Epoch[1/5]  [160/313]  eta: 0:00:30  Lr: 0.001875  Loss: 0.7448  Acc@1: 81.2500 (73.2143)  Acc@5: 100.0000 (94.0217)  time: 0.1847  data: 0.0001  max mem: 2372
Train: Epoch[1/5]  [170/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.7716  Acc@1: 81.2500 (73.7208)  Acc@5: 100.0000 (94.2251)  time: 0.1847  data: 0.0001  max mem: 2372
Train: Epoch[1/5]  [180/313]  eta: 0:00:26  Lr: 0.001875  Loss: 0.8106  Acc@1: 81.2500 (73.8605)  Acc@5: 100.0000 (94.4406)  time: 0.1848  data: 0.0001  max mem: 2372
Train: Epoch[1/5]  [190/313]  eta: 0:00:24  Lr: 0.001875  Loss: 0.8827  Acc@1: 81.2500 (74.4764)  Acc@5: 100.0000 (94.5681)  time: 0.1850  data: 0.0001  max mem: 2372
Train: Epoch[1/5]  [200/313]  eta: 0:00:22  Lr: 0.001875  Loss: 0.7882  Acc@1: 81.2500 (74.8756)  Acc@5: 93.7500 (94.4963)  time: 0.1853  data: 0.0001  max mem: 2372
Train: Epoch[1/5]  [210/313]  eta: 0:00:20  Lr: 0.001875  Loss: 0.6373  Acc@1: 81.2500 (75.0889)  Acc@5: 93.7500 (94.6386)  time: 0.1856  data: 0.0001  max mem: 2372
Train: Epoch[1/5]  [220/313]  eta: 0:00:18  Lr: 0.001875  Loss: 0.5467  Acc@1: 81.2500 (75.3394)  Acc@5: 100.0000 (94.7964)  time: 0.1853  data: 0.0001  max mem: 2372
Train: Epoch[1/5]  [230/313]  eta: 0:00:16  Lr: 0.001875  Loss: 0.8543  Acc@1: 81.2500 (75.4329)  Acc@5: 100.0000 (94.9675)  time: 0.1850  data: 0.0001  max mem: 2372
Train: Epoch[1/5]  [240/313]  eta: 0:00:14  Lr: 0.001875  Loss: 0.4893  Acc@1: 81.2500 (76.0633)  Acc@5: 100.0000 (95.1245)  time: 0.1849  data: 0.0001  max mem: 2372
Train: Epoch[1/5]  [250/313]  eta: 0:00:12  Lr: 0.001875  Loss: 0.2106  Acc@1: 87.5000 (76.3446)  Acc@5: 100.0000 (95.2191)  time: 0.1852  data: 0.0001  max mem: 2372
Train: Epoch[1/5]  [260/313]  eta: 0:00:10  Lr: 0.001875  Loss: 0.3212  Acc@1: 87.5000 (76.6523)  Acc@5: 100.0000 (95.3305)  time: 0.1855  data: 0.0001  max mem: 2372
Train: Epoch[1/5]  [270/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.0771  Acc@1: 81.2500 (76.7758)  Acc@5: 100.0000 (95.4566)  time: 0.1853  data: 0.0001  max mem: 2372
Train: Epoch[1/5]  [280/313]  eta: 0:00:06  Lr: 0.001875  Loss: 0.2075  Acc@1: 81.2500 (77.0685)  Acc@5: 100.0000 (95.6183)  time: 0.1852  data: 0.0001  max mem: 2372
Train: Epoch[1/5]  [290/313]  eta: 0:00:04  Lr: 0.001875  Loss: 0.3222  Acc@1: 87.5000 (77.1478)  Acc@5: 100.0000 (95.6186)  time: 0.1852  data: 0.0001  max mem: 2372
slurmstepd: error: *** JOB 924746 ON tg072 CANCELLED AT 2024-11-02T13:33:01 ***
=== JOB_STATISTICS ===
=== current date     : Sat 02 Nov 2024 01:33:04 PM CET
= Job-ID             : 924746 on tinygpu
= Job-Name           : l2p
= Job-Command        : /home/hpc/iwi1/iwi1102h/Backdoor/L2P_Backdoor/train_cifar100_l2p.sh
= Initial workdir    : /home/hpc/iwi1/iwi1102h/Backdoor/L2P_Backdoor
= Queue/Partition    : v100
= Slurm account      : iwi1 with QOS=normal
= Requested resources:  for 1-00:00:00
= Elapsed runtime    : 00:01:25
= Total RAM usage    : 2.1 GiB of requested  GiB (%)   
= Node list          : tg072
= Subm/Elig/Start/End: 2024-11-02T13:31:35 / 2024-11-02T13:31:35 / 2024-11-02T13:31:36 / 2024-11-02T13:33:01
======================
=== Quota infos ======
    Path              Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc           76.5G   104.9G   209.7G        N/A      80K     500K   1,000K        N/A    
    /home/vault          0.0K  1048.6G  2097.2G        N/A       1      200K     400K        N/A    
    /home/woody         17.9G  1000.0G  1500.0G        N/A     124K   5,000K   7,500K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
Tesla V100-PCIE-32GB, 00000000:18:00.0, 1536067, 78 %, 28 %, 3366 MiB, 68618 ms
