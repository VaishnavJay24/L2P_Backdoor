### Starting TaskPrologue of job 925012 on tg071 at Sun 03 Nov 2024 01:26:19 PM CET
Running on cores 4-5,12-13,20-21,28-29 with governor ondemand
Sun Nov  3 13:26:19 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           On  |   00000000:86:00.0 Off |                    0 |
| N/A   33C    P0             26W /  250W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

| distributed init (rank 0): env://
Files already downloaded and verified
Files already downloaded and verified
Creating original model: vit_base_patch16_224
Creating model: vit_base_patch16_224
Namespace(aa=None, batch_size=16, batchwise_prompt=True, clip_grad=1.0, color_jitter=None, cooldown_epochs=10, data_path='./local_datasets/', dataset='Split-CIFAR100', decay_epochs=30, decay_rate=0.1, device='cuda', dist_backend='nccl', dist_url='env://', distributed=False, drop=0.0, drop_path=0.0, embedding_key='cls', epochs=5, eval=False, freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], global_pool='token', gpu=0, head_type='prompt', initializer='uniform', input_size=224, length=5, lr=0.03, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, min_lr=1e-05, model='vit_base_patch16_224', momentum=0.9, nb_classes=100, num_tasks=10, num_workers=4, opt='adam', opt_betas=(0.9, 0.999), opt_eps=1e-08, output_dir='./output', patience_epochs=10, pin_mem=True, predefined_key='', pretrained=True, print_freq=10, prompt_key=True, prompt_key_init='uniform', prompt_pool=True, pull_constraint=True, pull_constraint_coeff=0.1, rank=0, recount=1, reinit_optimizer=True, remode='pixel', reprob=0.0, sched='constant', seed=42, shared_prompt_key=False, shared_prompt_pool=False, shuffle=False, size=10, smoothing=0.1, subparser_name='cifar100_l2p', task_inc=False, top_k=5, train_interpolation='bicubic', train_mask=True, unscale_lr=True, use_prompt_mask=False, warmup_epochs=5, warmup_lr=1e-06, weight_decay=0.0, world_size=1)
number of params: 122980
Start training for 5 epochs
Test: [Task 1]  [ 0/63]  eta: 0:02:19  Loss: 4.6194 (4.6194)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 2.2146  data: 0.6173  max mem: 828
Test: [Task 1]  [10/63]  eta: 0:00:16  Loss: 4.5944 (4.5939)  Acc@1: 0.0000 (0.5682)  Acc@5: 6.2500 (6.2500)  time: 0.3056  data: 0.0563  max mem: 829
Test: [Task 1]  [20/63]  eta: 0:00:09  Loss: 4.5942 (4.5956)  Acc@1: 0.0000 (0.8929)  Acc@5: 6.2500 (7.1429)  time: 0.1147  data: 0.0002  max mem: 829
Test: [Task 1]  [30/63]  eta: 0:00:06  Loss: 4.5935 (4.5916)  Acc@1: 0.0000 (0.8065)  Acc@5: 6.2500 (6.4516)  time: 0.1148  data: 0.0002  max mem: 829
Test: [Task 1]  [40/63]  eta: 0:00:03  Loss: 4.5979 (4.5956)  Acc@1: 0.0000 (0.7622)  Acc@5: 6.2500 (6.7073)  time: 0.1148  data: 0.0002  max mem: 829
Test: [Task 1]  [50/63]  eta: 0:00:02  Loss: 4.6026 (4.5943)  Acc@1: 0.0000 (0.7353)  Acc@5: 6.2500 (6.7402)  time: 0.1148  data: 0.0002  max mem: 829
Test: [Task 1]  [60/63]  eta: 0:00:00  Loss: 4.5811 (4.5928)  Acc@1: 0.0000 (1.1270)  Acc@5: 6.2500 (7.0697)  time: 0.1148  data: 0.0002  max mem: 829
Test: [Task 1]  [62/63]  eta: 0:00:00  Loss: 4.5882 (4.5931)  Acc@1: 0.0000 (1.1000)  Acc@5: 6.2500 (7.0000)  time: 0.1134  data: 0.0002  max mem: 829
Test: [Task 1] Total time: 0:00:09 (0.1489 s / it)
* Acc@1 1.100 Acc@5 7.000 loss 4.593
Test: [Task 1]  [ 0/63]  eta: 0:00:29  ASR: 0.0000 (0.0000)  ACC: 0.0000 (0.0000)  Loss: 4.6194 (4.6194)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.4719  data: 0.1810  max mem: 829
Test: [Task 1]  [10/63]  eta: 0:00:07  ASR: 0.0000 (1.1364)  ACC: 0.0000 (0.0000)  Loss: 4.5944 (4.5939)  Acc@1: 0.0000 (0.5682)  Acc@5: 6.2500 (6.2500)  time: 0.1491  data: 0.0167  max mem: 829
Test: [Task 1]  [20/63]  eta: 0:00:05  ASR: 0.0000 (0.5952)  ACC: 0.0000 (0.0000)  Loss: 4.5942 (4.5956)  Acc@1: 0.0000 (0.8929)  Acc@5: 6.2500 (7.1429)  time: 0.1167  data: 0.0002  max mem: 829
Test: [Task 1]  [30/63]  eta: 0:00:04  ASR: 0.0000 (0.7258)  ACC: 0.0000 (0.0000)  Loss: 4.5935 (4.5916)  Acc@1: 0.0000 (0.8065)  Acc@5: 6.2500 (6.4516)  time: 0.1166  data: 0.0002  max mem: 829
Test: [Task 1]  [40/63]  eta: 0:00:02  ASR: 0.0000 (1.2155)  ACC: 0.0000 (0.0000)  Loss: 4.5979 (4.5956)  Acc@1: 0.0000 (0.7622)  Acc@5: 6.2500 (6.7073)  time: 0.1166  data: 0.0003  max mem: 829
Test: [Task 1]  [50/63]  eta: 0:00:01  ASR: 0.0000 (0.9772)  ACC: 0.0000 (0.0000)  Loss: 4.6026 (4.5943)  Acc@1: 0.0000 (0.7353)  Acc@5: 6.2500 (6.7402)  time: 0.1165  data: 0.0003  max mem: 829
Test: [Task 1]  [60/63]  eta: 0:00:00  ASR: 0.0000 (1.1631)  ACC: 0.0000 (0.6410)  Loss: 4.5811 (4.5928)  Acc@1: 0.0000 (1.1270)  Acc@5: 6.2500 (7.0697)  time: 0.1164  data: 0.0002  max mem: 829
Test: [Task 1]  [62/63]  eta: 0:00:00  ASR: 0.0000 (1.1351)  ACC: 0.0000 (0.6270)  Loss: 4.5882 (4.5931)  Acc@1: 0.0000 (1.1000)  Acc@5: 6.2500 (7.0000)  time: 0.1136  data: 0.0002  max mem: 829
Test: [Task 1] Total time: 0:00:07 (0.1223 s / it)
* Acc@1 1.100 Acc@5 7.000 loss 4.593
* Acc@1 1.135 ASR 0.627
[Average accuracy till task1]	Acc@1: 1.1000	Acc@5: 7.0000	Loss: 4.5931
[rank0]: Traceback (most recent call last):
[rank0]:   File "main.py", line 165, in <module>
[rank0]:     main(args)
[rank0]:   File "main.py", line 135, in main
[rank0]:     train_and_evaluate(model, model_without_ddp, original_model,
[rank0]:   File "/home/hpc/iwi1/iwi1102h/Backdoor/L2P_Backdoor/engine.py", line 334, in train_and_evaluate
[rank0]:     'epoch': epoch,
[rank0]: NameError: name 'epoch' is not defined
/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  main()
E1103 13:27:03.585709 140445622437696 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 2661538) of binary: /home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/bin/python
Traceback (most recent call last):
  File "/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/site-packages/torch/distributed/launch.py", line 208, in <module>
    main()
  File "/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/site-packages/typing_extensions.py", line 2853, in wrapper
    return arg(*args, **kwargs)
  File "/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/site-packages/torch/distributed/launch.py", line 204, in main
    launch(args)
  File "/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/site-packages/torch/distributed/launch.py", line 189, in launch
    run(args)
  File "/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-11-03_13:27:03
  host      : tg071.rrze.uni-erlangen.de
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2661538)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
=== JOB_STATISTICS ===
=== current date     : Sun 03 Nov 2024 01:27:04 PM CET
= Job-ID             : 925012 on tinygpu
= Job-Name           : l2p
= Job-Command        : /home/hpc/iwi1/iwi1102h/Backdoor/L2P_Backdoor/train_cifar100_l2p.sh
= Initial workdir    : /home/hpc/iwi1/iwi1102h/Backdoor/L2P_Backdoor
= Queue/Partition    : v100
= Slurm account      : iwi1 with QOS=normal
= Requested resources:  for 1-00:00:00
= Elapsed runtime    : 00:00:50
= Total RAM usage    : 1.8 GiB of requested  GiB (%)   
= Node list          : tg071
= Subm/Elig/Start/End: 2024-11-03T13:26:13 / 2024-11-03T13:26:13 / 2024-11-03T13:26:13 / 2024-11-03T13:27:03
======================
=== Quota infos ======
    Path              Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc           83.2G   104.9G   209.7G        N/A      80K     500K   1,000K        N/A    
    /home/vault          0.0K  1048.6G  2097.2G        N/A       1      200K     400K        N/A    
    /home/woody         17.9G  1000.0G  1500.0G        N/A     124K   5,000K   7,500K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
Tesla V100-PCIE-32GB, 00000000:86:00.0, 2661538, 46 %, 15 %, 1874 MiB, 30518 ms
