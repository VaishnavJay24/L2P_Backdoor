### Starting TaskPrologue of job 925325 on tg071 at Mon 04 Nov 2024 07:55:45 AM CET
Running on cores 0-1,8-9,17-18,24-25 with governor ondemand
Mon Nov  4 07:55:45 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           On  |   00000000:18:00.0 Off |                    0 |
| N/A   31C    P0             27W /  250W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

| distributed init (rank 0): env://
Files already downloaded and verified
Files already downloaded and verified
Creating original model: vit_base_patch16_224
Creating model: vit_base_patch16_224
Namespace(aa=None, batch_size=16, batchwise_prompt=True, clip_grad=1.0, color_jitter=None, cooldown_epochs=10, data_path='./local_datasets/', dataset='Split-CIFAR100', decay_epochs=30, decay_rate=0.1, device='cuda', dist_backend='nccl', dist_url='env://', distributed=False, drop=0.0, drop_path=0.0, embedding_key='cls', epochs=5, eval=False, freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], global_pool='token', gpu=0, head_type='prompt', initializer='uniform', input_size=224, length=5, lr=0.03, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, min_lr=1e-05, model='vit_base_patch16_224', momentum=0.9, nb_classes=100, num_tasks=10, num_workers=4, opt='adam', opt_betas=(0.9, 0.999), opt_eps=1e-08, output_dir='./output', patience_epochs=10, pin_mem=True, predefined_key='', pretrained=True, print_freq=10, prompt_key=True, prompt_key_init='uniform', prompt_pool=True, pull_constraint=True, pull_constraint_coeff=0.1, rank=0, recount=1, reinit_optimizer=True, remode='pixel', reprob=0.0, sched='constant', seed=42, shared_prompt_key=False, shared_prompt_pool=False, shuffle=False, size=10, smoothing=0.1, subparser_name='cifar100_l2p', task_inc=False, top_k=5, train_interpolation='bicubic', train_mask=True, unscale_lr=True, use_prompt_mask=False, warmup_epochs=5, warmup_lr=1e-06, weight_decay=0.0, world_size=1)
number of params: 122980
Start training for 5 epochs
Train: Epoch[1/5]  [  0/313]  eta: 1:02:37  Loss: 2.2515 (2.2515)  ASR: 10.0000 (10.0000)  time: 12.0052  data: 0.5294  max mem: 2377
Train: Epoch[1/5]  [ 10/313]  eta: 0:06:24  Loss: 2.2559 (2.2561)  ASR: 27.2727 (25.3828)  time: 1.2674  data: 0.0485  max mem: 2381
Train: Epoch[1/5]  [ 20/313]  eta: 0:03:42  Loss: 2.2528 (2.2489)  ASR: 30.0000 (31.2417)  time: 0.1959  data: 0.0004  max mem: 2381
Train: Epoch[1/5]  [ 30/313]  eta: 0:02:43  Loss: 2.2271 (2.2396)  ASR: 38.4615 (35.5348)  time: 0.1958  data: 0.0004  max mem: 2381
Train: Epoch[1/5]  [ 40/313]  eta: 0:02:11  Loss: 2.2254 (2.2368)  ASR: 38.4615 (35.2828)  time: 0.1943  data: 0.0004  max mem: 2381
Train: Epoch[1/5]  [ 50/313]  eta: 0:01:52  Loss: 2.2172 (2.2319)  ASR: 38.4615 (37.8966)  time: 0.1950  data: 0.0004  max mem: 2382
Train: Epoch[1/5]  [ 60/313]  eta: 0:01:38  Loss: 2.2026 (2.2274)  ASR: 54.5455 (40.8253)  time: 0.1939  data: 0.0004  max mem: 2382
Train: Epoch[1/5]  [ 70/313]  eta: 0:01:27  Loss: 2.1977 (2.2218)  ASR: 57.1429 (42.9151)  time: 0.1930  data: 0.0004  max mem: 2382
Train: Epoch[1/5]  [ 80/313]  eta: 0:01:19  Loss: 2.1838 (2.2180)  ASR: 57.1429 (44.9288)  time: 0.1940  data: 0.0004  max mem: 2382
Train: Epoch[1/5]  [ 90/313]  eta: 0:01:12  Loss: 2.1874 (2.2151)  ASR: 53.8462 (45.7857)  time: 0.1944  data: 0.0004  max mem: 2382
Train: Epoch[1/5]  [100/313]  eta: 0:01:06  Loss: 2.1840 (2.2108)  ASR: 53.8462 (47.0586)  time: 0.1939  data: 0.0004  max mem: 2382
Train: Epoch[1/5]  [110/313]  eta: 0:01:01  Loss: 2.1803 (2.2087)  ASR: 64.2857 (48.9887)  time: 0.1942  data: 0.0004  max mem: 2382
Train: Epoch[1/5]  [120/313]  eta: 0:00:56  Loss: 2.1759 (2.2064)  ASR: 66.6667 (50.0322)  time: 0.1942  data: 0.0004  max mem: 2382
Train: Epoch[1/5]  [130/313]  eta: 0:00:52  Loss: 2.1759 (2.2044)  ASR: 66.6667 (51.3447)  time: 0.1944  data: 0.0004  max mem: 2382
Train: Epoch[1/5]  [140/313]  eta: 0:00:48  Loss: 2.1635 (2.2018)  ASR: 66.6667 (52.4872)  time: 0.1953  data: 0.0004  max mem: 2382
Train: Epoch[1/5]  [150/313]  eta: 0:00:44  Loss: 2.1749 (2.2010)  ASR: 66.6667 (52.7941)  time: 0.1946  data: 0.0004  max mem: 2382
Train: Epoch[1/5]  [160/313]  eta: 0:00:40  Loss: 2.1786 (2.1992)  ASR: 60.0000 (53.5941)  time: 0.1939  data: 0.0004  max mem: 2382
Train: Epoch[1/5]  [170/313]  eta: 0:00:37  Loss: 2.1646 (2.1975)  ASR: 72.7273 (54.5143)  time: 0.1950  data: 0.0004  max mem: 2382
Train: Epoch[1/5]  [180/313]  eta: 0:00:34  Loss: 2.1528 (2.1952)  ASR: 75.0000 (55.3556)  time: 0.1950  data: 0.0004  max mem: 2382
Train: Epoch[1/5]  [190/313]  eta: 0:00:31  Loss: 2.1528 (2.1934)  ASR: 73.3333 (56.2731)  time: 0.1943  data: 0.0004  max mem: 2382
Train: Epoch[1/5]  [200/313]  eta: 0:00:28  Loss: 2.1563 (2.1915)  ASR: 73.3333 (57.4811)  time: 0.1939  data: 0.0003  max mem: 2382
Train: Epoch[1/5]  [210/313]  eta: 0:00:25  Loss: 2.1493 (2.1892)  ASR: 78.5714 (58.6045)  time: 0.1937  data: 0.0003  max mem: 2382
Train: Epoch[1/5]  [220/313]  eta: 0:00:23  Loss: 2.1455 (2.1869)  ASR: 78.5714 (59.6539)  time: 0.1943  data: 0.0004  max mem: 2382
Train: Epoch[1/5]  [230/313]  eta: 0:00:20  Loss: 2.1500 (2.1854)  ASR: 81.8182 (60.7009)  time: 0.1949  data: 0.0004  max mem: 2382
Train: Epoch[1/5]  [240/313]  eta: 0:00:17  Loss: 2.1439 (2.1835)  ASR: 84.6154 (61.5638)  time: 0.1959  data: 0.0004  max mem: 2382
Train: Epoch[1/5]  [250/313]  eta: 0:00:15  Loss: 2.1325 (2.1814)  ASR: 92.3077 (62.6395)  time: 0.1950  data: 0.0004  max mem: 2382
Train: Epoch[1/5]  [260/313]  eta: 0:00:12  Loss: 2.1157 (2.1786)  ASR: 92.3077 (63.6262)  time: 0.1943  data: 0.0004  max mem: 2382
Train: Epoch[1/5]  [270/313]  eta: 0:00:10  Loss: 2.1052 (2.1758)  ASR: 92.3077 (64.7335)  time: 0.1947  data: 0.0004  max mem: 2382
Train: Epoch[1/5]  [280/313]  eta: 0:00:07  Loss: 2.0958 (2.1726)  ASR: 100.0000 (65.8754)  time: 0.1939  data: 0.0004  max mem: 2382
Train: Epoch[1/5]  [290/313]  eta: 0:00:05  Loss: 2.0665 (2.1684)  ASR: 100.0000 (66.8870)  time: 0.1935  data: 0.0003  max mem: 2382
Train: Epoch[1/5]  [300/313]  eta: 0:00:03  Loss: 2.0357 (2.1639)  ASR: 100.0000 (67.8843)  time: 0.1929  data: 0.0003  max mem: 2382
Train: Epoch[1/5]  [310/313]  eta: 0:00:00  Loss: 2.0252 (2.1589)  ASR: 100.0000 (68.8710)  time: 0.1925  data: 0.0003  max mem: 2382
Train: Epoch[1/5]  [312/313]  eta: 0:00:00  Loss: 2.0252 (2.1580)  ASR: 100.0000 (69.0204)  time: 0.4019  data: 0.0002  max mem: 2382
Train: Epoch[1/5] Total time: 0:01:16 (0.2457 s / it)
Averaged stats: Loss: 2.0252 (2.1580)  ASR: 100.0000 (69.0204)
Train: Epoch[2/5]  [  0/313]  eta: 0:02:10  Loss: 2.0440 (2.0440)  ASR: 100.0000 (100.0000)  time: 0.4185  data: 0.2135  max mem: 2382
Train: Epoch[2/5]  [ 10/313]  eta: 0:01:05  Loss: 1.9816 (1.9984)  ASR: 100.0000 (100.0000)  time: 0.2166  data: 0.0198  max mem: 2382
Train: Epoch[2/5]  [ 20/313]  eta: 0:01:00  Loss: 1.9766 (1.9818)  ASR: 100.0000 (99.6825)  time: 0.1950  data: 0.0004  max mem: 2383
Train: Epoch[2/5]  [ 30/313]  eta: 0:00:57  Loss: 1.9349 (1.9683)  ASR: 100.0000 (99.7850)  time: 0.1937  data: 0.0003  max mem: 2383
Train: Epoch[2/5]  [ 40/313]  eta: 0:00:54  Loss: 1.9487 (1.9715)  ASR: 100.0000 (99.8374)  time: 0.1939  data: 0.0003  max mem: 2383
Train: Epoch[2/5]  [ 50/313]  eta: 0:00:52  Loss: 1.9706 (1.9702)  ASR: 100.0000 (99.7292)  time: 0.1948  data: 0.0004  max mem: 2383
Train: Epoch[2/5]  [ 60/313]  eta: 0:00:50  Loss: 1.9524 (1.9645)  ASR: 100.0000 (99.4358)  time: 0.1959  data: 0.0004  max mem: 2383
Train: Epoch[2/5]  [ 70/313]  eta: 0:00:48  Loss: 1.9505 (1.9638)  ASR: 100.0000 (99.5152)  time: 0.1963  data: 0.0005  max mem: 2383
slurmstepd: error: *** JOB 925325 ON tg071 CANCELLED AT 2024-11-04T07:57:50 ***
=== JOB_STATISTICS ===
=== current date     : Mon 04 Nov 2024 07:57:53 AM CET
= Job-ID             : 925325 on tinygpu
= Job-Name           : l2p_0.1-poi_0.001-Opt_9-id_CE
= Job-Command        : /home/hpc/iwi1/iwi1102h/Backdoor/L2P_Backdoor/train_cifar100_l2p.sh
= Initial workdir    : /home/hpc/iwi1/iwi1102h/Backdoor/L2P_Backdoor
= Queue/Partition    : v100
= Slurm account      : iwi1 with QOS=normal
= Requested resources:  for 1-00:00:00
= Elapsed runtime    : 00:02:13
= Total RAM usage    : 2.2 GiB of requested  GiB (%)   
= Node list          : tg071
= Subm/Elig/Start/End: 2024-11-04T07:55:36 / 2024-11-04T07:55:36 / 2024-11-04T07:55:37 / 2024-11-04T07:57:50
======================
=== Quota infos ======
    Path              Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc           83.2G   104.9G   209.7G        N/A      80K     500K   1,000K        N/A    
    /home/vault          0.0K  1048.6G  2097.2G        N/A       1      200K     400K        N/A    
    /home/woody         17.9G  1000.0G  1500.0G        N/A     124K   5,000K   7,500K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
Tesla V100-PCIE-32GB, 00000000:18:00.0, 3066664, 78 %, 24 %, 4728 MiB, 106608 ms
