### Starting TaskPrologue of job 984184 on tg081 at Wed 29 Jan 2025 01:40:04 PM CET
Running on cores 6-7,22-23,38-39,54-55 with governor ondemand
Wed Jan 29 13:40:05 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3080        On  |   00000000:3E:00.0 Off |                  N/A |
| 30%   38C    P8             18W /  300W |       2MiB /  10240MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

| distributed init (rank 0): env://
Files already downloaded and verified
Files already downloaded and verified
Creating original model: vit_base_patch16_224
Creating model: vit_base_patch16_224
Namespace(aa=None, batch_size=16, batchwise_prompt=True, clip_grad=1.0, color_jitter=None, cooldown_epochs=10, data_path='./local_datasets/', dataset='Split-CIFAR100', decay_epochs=30, decay_rate=0.1, device='cuda', dist_backend='nccl', dist_url='env://', distributed=False, drop=0.0, drop_path=0.0, embedding_key='cls', epochs=5, eval=False, freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], global_pool='token', gpu=0, head_type='prompt', initializer='uniform', input_size=224, length=5, lr=0.03, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, min_lr=1e-05, model='vit_base_patch16_224', momentum=0.9, nb_classes=100, num_tasks=10, num_workers=4, opt='adam', opt_betas=(0.9, 0.999), opt_eps=1e-08, output_dir='./output', p_task_id=0, patience_epochs=10, pin_mem=True, poison_rate=0.1, predefined_key='', pretrained=True, print_freq=10, prompt_key=True, prompt_key_init='uniform', prompt_pool=True, pull_constraint=True, pull_constraint_coeff=0.1, rank=0, recount=1, reinit_optimizer=True, remode='pixel', reprob=0.0, sched='constant', seed=42, shared_prompt_key=False, shared_prompt_pool=False, shuffle=False, size=10, smoothing=0.1, subparser_name='cifar100_l2p', task_inc=False, top_k=5, train_interpolation='bicubic', train_mask=True, trigger_path='/home/woody/iwi1/iwi1102h/trigger/trigger_0_0.1_vit_base_patch16_224.pt', unscale_lr=True, use_prompt_mask=False, use_trigger=True, warmup_epochs=5, warmup_lr=1e-06, weight_decay=0.0, world_size=1)
number of params: 122980
Start training for 5 epochs
True
trigger loading
never here
0 0
[rank0]: Traceback (most recent call last):
[rank0]:   File "main.py", line 178, in <module>
[rank0]:     main(args)
[rank0]:   File "main.py", line 139, in main
[rank0]:     train_and_evaluate(model, model_without_ddp, original_model,
[rank0]:   File "/home/hpc/iwi1/iwi1102h/Backdoor/L2P_Backdoor_sleeper/engine.py", line 404, in train_and_evaluate
[rank0]:     train_stats,backdoor = train_one_epoch(model=model, original_model=original_model, criterion=criterion,
[rank0]:   File "/home/hpc/iwi1/iwi1102h/Backdoor/L2P_Backdoor_sleeper/engine.py", line 119, in train_one_epoch
[rank0]:     loss = backdoor.calculate_loss(criterion,original_model, model, input,target, task_id, class_mask,index)
[rank0]:   File "/home/hpc/iwi1/iwi1102h/Backdoor/L2P_Backdoor_sleeper/backdoor.py", line 238, in calculate_loss
[rank0]:     output_checker = model(self.input_checker, task_id=task_id, cls_features=cls_features)
[rank0]:   File "/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/hpc/iwi1/iwi1102h/Backdoor/L2P_Backdoor_sleeper/vision_transformer.py", line 514, in forward
[rank0]:     res = self.forward_features(x, task_id=task_id, cls_features=cls_features, train=train)
[rank0]:   File "/home/hpc/iwi1/iwi1102h/Backdoor/L2P_Backdoor_sleeper/vision_transformer.py", line 483, in forward_features
[rank0]:     x = self.blocks(x)
[rank0]:   File "/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/site-packages/torch/nn/modules/container.py", line 219, in forward
[rank0]:     input = module(input)
[rank0]:   File "/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/hpc/iwi1/iwi1102h/Backdoor/L2P_Backdoor_sleeper/vision_transformer.py", line 251, in forward
[rank0]:     x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))
[rank0]:   File "/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/hpc/iwi1/iwi1102h/Backdoor/L2P_Backdoor_sleeper/vision_transformer.py", line 213, in forward
[rank0]:     attn = (q @ k.transpose(-2, -1)) * self.scale
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 9.67 GiB of which 30.88 MiB is free. Including non-PyTorch memory, this process has 9.63 GiB memory in use. Of the allocated memory 7.65 GiB is allocated by PyTorch, and 1.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W129 13:40:43.904007567 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  main()
E0129 13:40:44.711279 140693963962176 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 509399) of binary: /home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/bin/python
Traceback (most recent call last):
  File "/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/site-packages/torch/distributed/launch.py", line 208, in <module>
    main()
  File "/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/site-packages/typing_extensions.py", line 2853, in wrapper
    return arg(*args, **kwargs)
  File "/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/site-packages/torch/distributed/launch.py", line 204, in main
    launch(args)
  File "/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/site-packages/torch/distributed/launch.py", line 189, in launch
    run(args)
  File "/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/woody/iwi1/iwi1102h/software/private/conda/envs/l2p/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-01-29_13:40:44
  host      : tg081.rrze.uni-erlangen.de
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 509399)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
=== JOB_STATISTICS ===
=== current date     : Wed 29 Jan 2025 01:40:45 PM CET
= Job-ID             : 984184 on tinygpu
= Job-Name           : trigger_0_0.1_vit_base_patch16_224
= Job-Command        : /home/hpc/iwi1/iwi1102h/Backdoor/L2P_Backdoor_sleeper/train_cifar100_l2p3080.sh
= Initial workdir    : /home/hpc/iwi1/iwi1102h/Backdoor/L2P_Backdoor_sleeper
= Queue/Partition    : work
= Slurm account      : iwi1 with QOS=normal
= Requested resources:  for 1-00:00:00
= Elapsed runtime    : 00:00:45
= Total RAM usage    : 5.7 GiB of requested  GiB (%)   
= Node list          : tg081
= Subm/Elig/Start/End: 2025-01-29T13:40:00 / 2025-01-29T13:40:00 / 2025-01-29T13:40:00 / 2025-01-29T13:40:45
======================
=== Quota infos ======
    Path              Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc           94.9G   104.9G   209.7G        N/A      85K     500K   1,000K        N/A    
    /home/woody         42.0G  1000.0G  1500.0G        N/A     269K   5,000K   7,500K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA GeForce RTX 3080, 00000000:3E:00.0, 509399, 4 %, 0 %, 9868 MiB, 28418 ms
